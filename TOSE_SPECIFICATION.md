##  Token-Optimized SQL Exchange (TOSE) Specification v1.0

### 1. Abstract

**Token-Optimized SQL Exchange (TOSE)** is a compact, human-readable serialization format designed to pipe structured, uniform query result sets from SQL databases (like PostgreSQL) to Large Language Models (LLMs) with significantly reduced token usage compared to JSON.

It achieves CSV-like compactness while prepending a single **Schema Header** that provides the explicit structural context (entity name, row count, and field names) required for reliable, lossless LLM parsing.

TOSE is optimized exclusively for **uniform arrays of records** (i.e., standard `SELECT` query results) and is intended to be generated by a lightweight, high-performance filter tool (e.g., `tose_converter` written in Rust) that consumes standard database CSV output.

### 2. Guiding Principles

1.  **Token Minimization:** Eliminate all repetitive per-row syntax, such as field names and structural delimiters (`{`, `}`), that make JSON inefficient for tabular data.
2.  **Structural Explicitness:** Provide a single, complete metadata header for the *entire dataset* so an LLM can parse the subsequent data block reliably.
3.  **Lossless Data Integrity:** Ensure that all data, including special characters (commas, quotes, newlines) and `NULL` values, is perfectly preserved.
4.  **Database-Native Encoding:** Leverage the robust, battle-tested **RFC 4180 CSV** format generated by database engines (`COPY ... FORMAT CSV`) for the data block, avoiding the need to re-invent complex escaping rules.
5.  **High Performance:** The format must be generatable at high speed via a simple stream filter, suitable for piping gigabytes of data.

-----

### 3. Format Structure

A valid TOSE document consists of exactly two parts, separated by a single newline character (`\n`):

1.  **The Schema Header** (One line)
2.  **The Data Block** (Zero or more lines)

<!-- end list -->

```
[Schema Header]\n
[Data Block]
```

#### 3.1. The Schema Header

The Schema Header is a single line that describes the entire data block. It provides all metadata required for an LLM to interpret the data.

**Syntax:**
`ENTITY_NAME[ROW_COUNT]{FIELD_1,FIELD_2,...,FIELD_N}:`

**Component Breakdown:**

| Component | Description | Example | Rules |
| :--- | :--- | :--- | :--- |
| **`ENTITY_NAME`** | A descriptive label for the data array. | `orderItems` | Must be a single alphanumeric string. `snake_case` or `camelCase` recommended. No spaces or special characters. |
| **`[` and `]`** | Delimiters for the row count. | `[3]` | Required. |
| **`ROW_COUNT`** | An integer representing the exact number of data rows in the Data Block. | `3` | Must be 0 or greater. |
| **`{` and `}`** | Delimiters for the field list. | `{sku,qty,price}` | Required. |
| **`FIELD_LIST`** | A comma-separated list of field (column) names. | `sku,qty,price` | Must not contain spaces around commas. The **order must exactly match** the column order in the Data Block. |
| **`:` (Terminator)** | The terminator for the Schema Header. | `:` | Required. Must be the final character of the line. |

**Example Header:**
`users[150]{id,username,email,last_login,is_active}:`

#### 3.2. The Data Block

The Data Block contains the raw data values, starting on the line immediately following the Schema Header.

  * **Row Delimiter:** Each record (row) is separated by a single newline character (`\n`).
  * **Field Delimiter:** Values (columns) within a row are separated by a single comma (`,`).
  * **Encoding:** The Data Block **must** adhere to the **RFC 4180 CSV** specification for data encoding and escaping.

**Example Data Block (for the header above):**

```
101,j.doe,j.doe@example.com,2025-11-06T09:30:00,true
102,a.smith,a.smith@example.com,2025-11-05T12:00:00,true
... (148 more rows)
```

-----

### 4. Data Serialization & Escaping

Data integrity is paramount. TOSE's Data Block **is** an RFC 4180 CSV block. This delegates escaping to the database engine.

| Data State | SQL Value (Example) | TOSE Data Block Representation (RFC 4180) |
| :--- | :--- | :--- |
| **Standard Value** | `ABC` | `ABC` |
| **`NULL` Value** | `NULL` | `` (empty field) |
| **Value with Comma** | `Widget, Blue` | `"Widget, Blue"` (Must be quoted) |
| **Value with Double Quote** | `Part "X"` | `"Part ""X"""` (Inner quote is doubled, field is quoted) |
| **Value with Newline** | `Line 1\nLine 2` | `"Line 1\nLine 2"` (Must be quoted, newline is preserved) |
| **Value w/ Comma & Quote** | `Item "A", Qty: 2` | `"Item ""A"", Qty: 2"` (Both rules apply) |

**Example of `NULL`:** A row `(101, NULL, 9.99)` would be serialized as:
`101,,9.99`

-----

### 5. Data Type Normalization

To ensure consistent LLM interpretation, data types should be normalized by the database engine's CSV converter.

  * **Numeric:** Simple string form. No thousands separators. (e.g., `12345.67`)
  * **Boolean:** Lowercase `true` or `false`.
  * **Temporal (Timestamp/Date):** **ISO 8601** format (e.g., `2025-11-06T09:30:00.123+11:00` or `2025-11-06`). This is the most unambiguous format for LLMs.
  * **UUID:** Standard 36-character string representation.
  * **Binary (e.g., `bytea`):** PostgreSQL's `HEX` format (e.g., `\xDEADBEEF...`).

-----

### 6. Recommended Implementation: `tose_converter` (Rust)

The TOSE specification is designed to be implemented as a simple, high-performance Rust CLI tool that acts as a pipe filter.

#### 6.1. Pipeline Workflow

The tool reads from `stdin` and writes to `stdout`, allowing for this flexible pipeline:

`[PSQL Command]` | `./tose_converter [ARGS]` | `[LLM API / File]`

1.  **`psql` (Source):** Generates the raw, RFC 4180-compliant data block.
    ```bash
    psql -d my_db -c "\copy (SELECT ...) TO STDOUT WITH (FORMAT CSV, HEADER FALSE)"
    ```
2.  **`tose_converter` (Filter):** Reads the `stdin` stream, counts the lines, prepends the generated Schema Header, and writes the full TOSE document to `stdout`.
3.  **Consumer (Destination):** The `stdout` stream is piped to the next process (e.g., a script calling an LLM API).

#### 6.2. CLI Specification

**Usage:**
`./tose_converter <ENTITY_NAME> <FIELD_1> <FIELD_2> ... <FIELD_N>`

  * **`ENTITY_NAME` (Arg 1):** The required entity name for the Schema Header.
  * **`FIELD_LIST` (Args 2..N):** The required, ordered list of field names.

#### 6.3. Core Rust Logic

1.  **Parse Arguments:** Collect `std::env::args()`. The first argument is `ENTITY_NAME`, and all subsequent arguments are the `FIELD_LIST`.
2.  **Read `stdin`:** Read the *entire* standard input stream into a single `String` buffer. This is the complete Data Block.
    ```rust
    use std::io::{self, Read};
    let mut data_buffer = String::new();
    io::stdin().read_to_string(&mut data_buffer)?;
    ```
3.  **Count Rows:** Count the number of newline-separated lines in the `data_buffer`.
    ```rust
    let row_count = data_buffer.trim().split('\n').filter(|s| !s.is_empty()).count();
    ```
4.  **Build Header:** Assemble the Schema Header string.
    ```rust
    let entity_name = &args[1];
    let field_list = &args[2..].join(",");
    let header = format!("{}[{}]{}:{}`", entity_name, row_count, field_list, '\n');
    ```
5.  **Write `stdout`:** Use a `BufWriter` for performance. Write the `header` string, then write the original `data_buffer` string.
    ```rust
    use std::io::Write;
    let stdout = io::stdout();
    let mut handle = io::BufWriter::new(stdout.lock());

    handle.write_all(header.as_bytes())?;
    handle.write_all(data_buffer.as_bytes())?;
    handle.flush()?;
    ```

-----

### 7. Full Examples

#### 7.1. Simple Example

**Database Table (`order_items`):**
| sku | qty | price |
| :--- | :--- | :--- |
| A1 | 2 | 9.99 |
| B2 | 1 | 14.50 |

**Command:**

```bash
psql -d my_db -c "\copy (SELECT sku, qty, price FROM order_items) TO STDOUT WITH (FORMAT CSV, HEADER FALSE)" \
| ./tose_converter orderItems sku qty price
```

**Final TOSE Output (to `stdout`):**

```
orderItems[2]{sku,qty,price}:
A1,2,9.99
B2,1,14.50
```

#### 7.2. Complex Example (with NULL, Quotes, and Commas)

**Database Table (`notes`):**
| id | title | body |
| :--- | :--- | :--- |
| 1 | Note "A" | `NULL` |
| 2 | "Comma, Inc." | A simple note. |

**Command:**

```bash
psql -d my_db -c "\copy (SELECT id, title, body FROM notes) TO STDOUT WITH (FORMAT CSV, HEADER FALSE)" \
| ./tose_converter notes id title body
```

**Final TOSE Output (to `stdout`):**

```
notes[2]{id,title,body}:
1,"Note ""A"",""
2,"""Comma, Inc.""",A simple note.
```